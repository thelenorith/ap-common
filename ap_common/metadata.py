"""
Metadata extraction, enrichment, and filtering functions.

Generated By: Cursor (Claude Sonnet 4.5)
"""

from ap_common.fits import get_fits_headers, get_xisf_headers, get_file_headers
from ap_common.progress import progress_iter, ProgressTracker
from ap_common.utils import build_profile, get_filenames


def get_metadata(
    dirs: list,
    profileFromPath: bool,
    patterns: list = None,
    recursive: bool = False,
    required_properties: list = None,
    debug: bool = False,
    printStatus: bool = False,
    latitude: str = None,
    longitude: str = None,
):
    """
    Loads metadata for files in the given directories, ensuring all required properties are present.
    Optionally prints status updates.

    Args:
        dirs: List of directories to search
        profileFromPath: Whether to extract profile from path
        patterns: List of regex patterns to match (defaults to [r".*[.]fits$"])
        recursive: Whether to search recursively
        required_properties: List of properties that must be present in metadata
        debug: Print debug information
        printStatus: Print status updates during processing
        latitude: Optional latitude override for files that don't have location data (e.g., CR2 files)
        longitude: Optional longitude override for files that don't have location data (e.g., CR2 files)

    Returns:
        Dictionary mapping filenames to their metadata dictionaries

    Raises:
        ValueError: If latitude or longitude is required (in required_properties) but not found
            in file headers and not provided as parameters
    """
    if patterns is None:
        patterns = [r".*\.fits$"]
    if required_properties is None:
        required_properties = []

    _required_properties = list(required_properties)
    # 'targetname' is always required, simply to have a value of None...
    if "targetname" not in _required_properties:
        _required_properties.append("targetname")

    # key of 'data' is the full path of the file
    data = {}

    # find files and load metadata from path+name.
    filenames = get_filenames(
        dirs=dirs,
        patterns=patterns,
        recursive=recursive,
    )

    for filename in progress_iter(
        filenames, desc="Loading metadata", enabled=printStatus
    ):
        d = get_file_headers(filename, profileFromPath=profileFromPath)
        data[d["filename"]] = d

    # make sure all required properties are at least None
    for f in data.keys():
        for p in _required_properties:
            if p not in data[f]:
                data[f][p] = None

    return enrich_metadata(
        data=data,
        required_properties=_required_properties,
        debug=debug,
        printStatus=printStatus,
        profileFromPath=profileFromPath,
        latitude=latitude,
        longitude=longitude,
    )


def enrich_metadata(
    data: dict,
    profileFromPath: bool,
    required_properties: list = None,
    debug: bool = False,
    printStatus: bool = False,
    latitude: str = None,
    longitude: str = None,
):
    """
    Enriches metadata for files missing required properties by extracting additional headers from the files themselves.
    Optionally prints status updates.

    Args:
        data: Dictionary mapping filenames to metadata dictionaries
        profileFromPath: Whether to extract profile from path
        required_properties: List of properties that must be present
        debug: Print debug information
        printStatus: Print status updates during processing
        latitude: Optional latitude override for files that don't have location data (e.g., CR2 files)
        longitude: Optional longitude override for files that don't have location data (e.g., CR2 files)

    Returns:
        Dictionary with enriched metadata

    Raises:
        ValueError: If latitude or longitude is required (in required_properties) but not found
            in file headers and not provided as parameters
    """
    if required_properties is None:
        required_properties = []

    # list of filenames (key of data dict) that need enrichment
    to_enrich = []

    # check each datum for enrichment
    for datum in data.values():
        # check if we have all required properties
        for rp in required_properties:
            if rp not in datum or datum[rp] is None or len(str(datum[rp])) == 0:
                # required property is missing, must enrich.
                to_enrich.append(datum["filename"])
                continue

    to_enrich.sort()

    # enrich things that need it
    with ProgressTracker(
        to_enrich, desc="Enriching metadata", enabled=printStatus
    ) as tracker:
        for filename in tracker:
            datum = data[filename]
            # get headers from metadata.  normalize and use file naming override.
            enriched = None
            if filename.endswith(".fits"):
                enriched = get_fits_headers(
                    filename,
                    normalize=True,
                    file_naming_override=True,
                    profileFromPath=profileFromPath,
                )
            elif filename.endswith(".xisf"):
                enriched = get_xisf_headers(
                    filename,
                    normalize=True,
                    file_naming_override=True,
                    profileFromPath=profileFromPath,
                )
            else:
                # some other file type, probably cr2
                # Location not available from file headers
                # Use provided latitude/longitude if available, otherwise leave unset
                if latitude is not None:
                    datum["latitude"] = latitude
                if longitude is not None:
                    datum["longitude"] = longitude
                # we can do no more, treat datum as if it were enriched.
                enriched = datum

            # Build profile and add to enriched data
            enriched["profile"] = build_profile(enriched)

            # Update progress bar status with current target if available
            if "targetname" in enriched and enriched["targetname"]:
                tracker.set_status(enriched["targetname"])

            # store the now-enriched data
            data[filename] = enriched

    # make sure 'filename' is always set.  enrich will strip it.
    for filename in data.keys():
        data[filename]["filename"] = filename

    # Validate that required properties are present, especially location if required
    for filename in data.keys():
        datum = data[filename]
        for rp in required_properties:
            if rp in ["latitude", "longitude"]:
                if rp not in datum or datum[rp] is None or len(str(datum[rp])) == 0:
                    raise ValueError(
                        f"Required property '{rp}' is missing for file '{filename}'. "
                        f"Location data is not available from file headers. "
                        f"Please provide latitude and/or longitude parameters to enrich_metadata() "
                        f"or ensure the file contains location information in its headers."
                    )

    return data


def filter_metadata(data: dict, filters: dict, debug: bool = False):
    """
    Filters a metadata dictionary based on provided filter key/value pairs or functions.
    Returns a new dictionary with only matching entries.

    Args:
        data: Dictionary mapping filenames to metadata dictionaries
        filters: Dictionary of filter key/value pairs or functions
        debug: Print debug information

    Returns:
        Filtered dictionary with only matching entries
    """
    # validate input filter data
    if filters is None or len(filters) == 0:
        raise ValueError("Invalid filter data")

    # validate filter values
    for filter_key in filters.keys():
        filter_value = filters[filter_key]

        if filter_value is None:
            # unexpected.  bad input, but it should be rejected.
            print(f"ERROR filter: key '{filter_key}' has no value '{filter_value}'")
            raise ValueError(f"filter key '{filter_key}' has no value '{filter_value}'")

    # filters are good.  process the data and build a new output data set
    output = {}

    # for each datum, check filter.  if it matches all filters, add the datum to 'new_data'
    for filename in data.keys():
        datum = data[filename]
        # process each filter for this datum
        # is_match will be False if at least one filter does not match
        is_match = True

        # loop through each filter.  if any filter does not match set is_match False and break the loop
        for filter_key in filters.keys():
            filter_value = filters[filter_key]

            # if we don't have the filter in the datum it's ok, just treat it as "OK"
            if filter_key not in datum:
                continue

            # filter exists in datum, check value
            if callable(filter_value):
                try:
                    # assumes the function returns bool...
                    if not filter_value(datum[filter_key]):
                        # not a match
                        is_match = False
                        break
                except Exception as e:
                    # no idea, bad function? bail!
                    raise RuntimeError(
                        f"WARNING failed to call function '{filter_value}' with argument '{datum[filter_key]}': {e}"
                    )

            elif isinstance(filter_value, int):
                try:
                    # convert to float first because "90.00" won't convert to int directly
                    if int(float(datum[filter_key])) != filter_value:
                        # not a match
                        is_match = False
                        break
                except (ValueError, TypeError):
                    # cannot convert to int probably, so it's not a match
                    is_match = False
                    break

            elif isinstance(filter_value, float):
                try:
                    # set to match if
                    if float(datum[filter_key]) != filter_value:
                        # not a match
                        is_match = False
                        break
                except (ValueError, TypeError):
                    # cannot convert to float probably, so it's not a match
                    is_match = False
                    break

            else:
                # default, treat as string
                if str(datum[filter_key]) != filter_value:
                    # not a match
                    is_match = False
                    break

        # all filters have been checked, did we find a match?
        if is_match:
            # found a match for all filters.  add datum to 'output'
            output[filename] = datum

    return output


def get_filtered_metadata(
    dirs: list,
    filters: dict,
    profileFromPath: bool,
    patterns: list = None,
    recursive: bool = False,
    required_properties: list = None,
    debug: bool = False,
    printStatus: bool = False,
    latitude: str = None,
    longitude: str = None,
):
    """
    Loads metadata for files in given directories, then filters the metadata based on provided filters and required properties.

    Args:
        dirs: List of directories to search
        filters: Dictionary of filter key/value pairs or functions
        profileFromPath: Whether to extract profile from path
        patterns: List of regex patterns to match (defaults to [r".*[.]fits$"])
        recursive: Whether to search recursively
        required_properties: List of properties that must be present
        debug: Print debug information
        printStatus: Print status updates during processing
        latitude: Optional latitude override for files that don't have location data (e.g., CR2 files)
        longitude: Optional longitude override for files that don't have location data (e.g., CR2 files)

    Returns:
        Filtered dictionary with only matching entries

    Raises:
        ValueError: If latitude or longitude is required (in required_properties) but not found
            in file headers and not provided as parameters
    """
    if patterns is None:
        patterns = [r".*\.fits$"]
    if required_properties is None:
        required_properties = []

    for filter in filters.keys():
        if filter not in required_properties:
            required_properties.append(filter)

    metadata = get_metadata(
        dirs=dirs,
        patterns=patterns,
        recursive=recursive,
        required_properties=required_properties,
        debug=debug,
        printStatus=printStatus,
        profileFromPath=profileFromPath,
        latitude=latitude,
        longitude=longitude,
    )

    metadata = filter_metadata(
        data=metadata,
        filters=filters,
        debug=debug,
    )

    return metadata


def group_by_directory(data: dict) -> dict:
    """
    Groups metadata entries by their parent directory.

    Args:
        data: Dictionary mapping filenames to metadata dictionaries

    Returns:
        Dictionary mapping directory paths to dictionaries of {filename: metadata}
        for files in that directory
    """
    import os

    grouped = {}

    for filename, metadata in data.items():
        directory = os.path.dirname(filename)
        if directory not in grouped:
            grouped[directory] = {}
        grouped[directory][filename] = metadata

    return grouped


def get_directories_with_lights(data: dict) -> set:
    """
    Returns the set of directories that contain at least one LIGHT frame.

    This is useful for finding directories that may need calibration frames
    to be matched with their light frames.

    Args:
        data: Dictionary mapping filenames to metadata dictionaries

    Returns:
        Set of directory paths that contain at least one LIGHT type file
    """
    import os
    from ap_common.constants import TYPE_LIGHT, NORMALIZED_HEADER_TYPE

    directories = set()

    for filename, metadata in data.items():
        frame_type = metadata.get(NORMALIZED_HEADER_TYPE, "")
        if frame_type and frame_type.upper() == TYPE_LIGHT:
            directories.add(os.path.dirname(filename))

    return directories


def get_calibration_candidates(
    data: dict,
    light_directories: set = None,
    calibration_types: list = None,
) -> dict:
    """
    Returns calibration frames that could potentially be used for calibration
    of light frames in the specified directories.

    Calibration frames are matched based on being in the same directory as
    light frames. This helps identify which calibration frames exist for
    a given set of light frame directories.

    Args:
        data: Dictionary mapping filenames to metadata dictionaries
        light_directories: Set of directories to find calibration frames for.
            If None, uses get_directories_with_lights(data) to find directories.
        calibration_types: List of calibration frame types to look for.
            If None, uses ALL_CALIBRATION_TYPES from constants.

    Returns:
        Dictionary mapping filenames to metadata for calibration frames
        found in the specified directories
    """
    from ap_common.constants import ALL_CALIBRATION_TYPES, NORMALIZED_HEADER_TYPE
    import os

    if calibration_types is None:
        calibration_types = ALL_CALIBRATION_TYPES

    if light_directories is None:
        light_directories = get_directories_with_lights(data)

    calibration_frames = {}

    for filename, metadata in data.items():
        directory = os.path.dirname(filename)
        if directory in light_directories:
            frame_type = metadata.get(NORMALIZED_HEADER_TYPE, "")
            if frame_type and frame_type.upper() in [ct.upper() for ct in calibration_types]:
                calibration_frames[filename] = metadata

    return calibration_frames
